{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "from random import seed, sample"
   ],
   "id": "88f5ebee49e84029",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load the Data",
   "id": "ea0640d76263dee4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the datasets, ignoring the first column\n",
    "train_data = pd.read_csv('train.csv',\n",
    "                         usecols=lambda column: column not in ['Unnamed: 0'])  # Adjust 'Unnamed: 0' if necessary\n",
    "test_data = pd.read_csv('test.csv',\n",
    "                        usecols=lambda column: column not in ['Unnamed: 0'])  # Adjust 'Unnamed: 0' if necessary\n",
    "\n",
    "X_train = train_data[['x', 'y']].values\n",
    "y_train = train_data['cls'].values\n",
    "\n",
    "X_test = test_data[['x', 'y']].values\n",
    "y_test = test_data['cls'].values"
   ],
   "id": "47db5711bc0cd644",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualize the Data",
   "id": "1df144b4c60cd7b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set the style of seaborn\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=train_data, x='x', y='y', hue='cls', palette='deep', alpha=0.7)\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Scatter Plot of Training Data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend(title='Class')\n",
    "plt.show()\n"
   ],
   "id": "29482eada4acdccb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analyze the Data",
   "id": "d5cb2389785ef039"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check the distribution of classes\n",
    "class_distribution = train_data['cls'].value_counts()\n",
    "print(class_distribution)\n",
    "\n",
    "# Summary statistics of the features\n",
    "print(train_data.describe())\n"
   ],
   "id": "a017c01344ee2edc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# evaluate model",
   "id": "8b6a53b914965582"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Predict on training data\n",
    "    y_train_pred = model.predict(X_train)\n",
    "\n",
    "    # Calculate metrics for training data\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "    train_precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "    train_recall = recall_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "    print(\"Training Metrics:\")\n",
    "    print(f\"Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {train_f1:.4f}\")\n",
    "    print(f\"Precision: {train_precision:.4f}\")\n",
    "    print(f\"Recall: {train_recall:.4f}\")\n",
    "    print()\n",
    "\n",
    "    # Predict on testing data\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics for testing data\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "    test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "    test_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "    print(\"Testing Metrics:\")\n",
    "    print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {test_f1:.4f}\")\n",
    "    print(f\"Precision: {test_precision:.4f}\")\n",
    "    print(f\"Recall: {test_recall:.4f}\")\n",
    "    print()\n"
   ],
   "id": "9e5e2494199b82cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# hyper parameter tuning",
   "id": "977261716d28de02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def tune_hyperparameters(model, param_grid, X_train, y_train, scoring='accuracy', cv=5):\n",
    "    print(\"Starting hyperparameter tuning...\")\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=cv, n_jobs=4, verbose=1)\n",
    "\n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best model and parameters\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(\"\\nHyperparameter Tuning Complete!\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best Cross-Validation Score: {best_score:.4f}\")\n",
    "\n",
    "    return best_model\n"
   ],
   "id": "a0c0b3a61ba6f4af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Param grids",
   "id": "8de35e0f8f417202"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "param_grid_bagging = {\n",
    "    'n_estimators': [10, 20, 50],\n",
    "    'max_samples': [0.5, 0.7, 1.0],\n",
    "    'max_features': [0.5, 0.7, 1.0],\n",
    "    'bootstrap': [True, False],\n",
    "    'estimator': [DecisionTreeClassifier(max_depth=5), RandomForestClassifier(n_estimators=10, max_depth=5)]\n",
    "}\n",
    "\n",
    "param_grid_bagging_custom = {\n",
    "    'n_estimators': [5, 10, 20],  # Number of base estimators\n",
    "    'max_depth': [3, 5, 7],  # Depth of each base tree\n",
    "    'min_samples_split': [5, 10, 15],  # Minimum samples to split\n",
    "    'max_samples': [0.6, 0.8, 1.0],  # Proportion of samples per bootstrap\n",
    "    'base_model': [DecisionTreeClassifier, RandomForestClassifier]\n",
    "}\n",
    "\n",
    "param_grid_adaboost = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'learning_rate': [0.01, 0.1, 1.0],\n",
    "    'estimator': [DecisionTreeClassifier(max_depth=3)]\n",
    "}\n",
    "\n",
    "param_grid_random_forest = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'min_samples_split': [5, 10],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "param_grid_random_forest_custom = {\n",
    "    'n_estimators': [5, 10, 20],  # Number of trees in the forest\n",
    "    'max_depth': [3, 5, 7],  # Depth of individual trees\n",
    "    'min_samples_split': [5, 10, 15],  # Minimum samples required to split a node\n",
    "    'max_features': ['sqrt', 'log2', 0.5, 0.7]  # Features considered at each split\n",
    "}\n"
   ],
   "id": "174b8ff58cd2882a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# phase1: Bagging",
   "id": "32b3d1787a65b944"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Bagging:\n",
    "    def __init__(self, base_model=DecisionTreeClassifier, n_estimators=10, max_depth=5, min_samples_split=5,\n",
    "                 max_samples=0.7, random_state=42):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_samples = max_samples\n",
    "        self.random_state = random_state\n",
    "        self.base_model = base_model\n",
    "        self.models = []\n",
    "        self.bootstraps = []\n",
    "\n",
    "        seed(self.random_state)\n",
    "\n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        n_samples = int(len(y) * self.max_samples)\n",
    "        indices = sample(range(len(y)), n_samples)\n",
    "        return X[indices], y[indices]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.models = []\n",
    "        for _ in range(self.n_estimators):\n",
    "            X_bootstrap, y_bootstrap = self._bootstrap_sample(X, y)\n",
    "            tree = self.base_model(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            tree.fit(X_bootstrap, y_bootstrap)\n",
    "            self.models.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([model.predict(X) for model in self.models])\n",
    "        return np.apply_along_axis(lambda x: Counter(x).most_common(1)[0][0], axis=0, arr=predictions)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'base_model': self.base_model,\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'random_state': self.random_state\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n"
   ],
   "id": "2a85f07aac0a1c0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train Bagging ensemble\n",
    "custom_bagging = tune_hyperparameters(Bagging(), param_grid_bagging_custom, X_train, y_train)\n",
    "\n",
    "# Train 5 individual Decision Trees\n",
    "trees = []\n",
    "for i in range(5):\n",
    "    tree = DecisionTreeClassifier(random_state=42 + i)\n",
    "    tree.fit(X_train, y_train)\n",
    "    trees.append(tree)\n",
    "\n",
    "\n",
    "# Define a function to plot decision boundaries\n",
    "def plot_decision_boundary(model, X, y, title, ax):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=0.8, cmap='Paired')\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap='Paired')\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "# Plot decision boundaries\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Plot each Decision Tree\n",
    "for i, tree in enumerate(trees):\n",
    "    plot_decision_boundary(tree, X_test, y_test, f\"Tree {i + 1}\", axes[i])\n",
    "\n",
    "# Plot Bagging model\n",
    "plot_decision_boundary(custom_bagging, X_test, y_test, \"Bagging\", axes[-1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "fc2a34454d2a04f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Train Bagging ensemble using sklearn's BaggingClassifier\n",
    "sklearn_bagging = BaggingClassifier()\n",
    "sklearn_bagging = tune_hyperparameters(sklearn_bagging, param_grid_bagging, X_train, y_train)\n",
    "sklearn_bagging.fit(X_train, y_train)\n",
    "\n",
    "evaluate_model(custom_bagging, X_train, y_train, X_test, y_test)\n",
    "evaluate_model(sklearn_bagging, X_train, y_train, X_test, y_test)\n"
   ],
   "id": "28861bc5567a5eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Phase 2",
   "id": "74e8a024d538ae2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, n_estimators=10, max_depth=5, min_samples_split=5, max_features='sqrt', random_state=42):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.models = []\n",
    "        self.bootstraps = []\n",
    "\n",
    "        seed(self.random_state)\n",
    "\n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        n_samples = len(y)\n",
    "        indices = sample(range(n_samples), n_samples)\n",
    "        return X[indices], y[indices]\n",
    "\n",
    "    def _get_max_features(self, n_features):\n",
    "        if self.max_features == 'sqrt':\n",
    "            return max(1, int(np.sqrt(n_features)))\n",
    "        elif self.max_features == 'log2':\n",
    "            return max(1, int(np.log2(n_features)))\n",
    "        elif isinstance(self.max_features, int):\n",
    "            return self.max_features\n",
    "        else:\n",
    "            return n_features\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.models = []\n",
    "        n_features = X.shape[1]\n",
    "        max_features = self._get_max_features(n_features)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            X_bootstrap, y_bootstrap = self._bootstrap_sample(X, y)\n",
    "            feature_indices = sample(range(n_features), max_features)\n",
    "            X_subset = X_bootstrap[:, feature_indices]\n",
    "\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            tree.fit(X_subset, y_bootstrap)\n",
    "            self.models.append((tree, feature_indices))\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for tree, feature_indices in self.models:\n",
    "            X_subset = X[:, feature_indices]\n",
    "            predictions.append(tree.predict(X_subset))\n",
    "\n",
    "        predictions = np.array(predictions)\n",
    "        return np.apply_along_axis(lambda x: Counter(x).most_common(1)[0][0], axis=0, arr=predictions)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'random_state': self.random_state,\n",
    "            'max_features': self.max_features,\n",
    "            'max_depth': self.max_depth,\n",
    "            'min_samples_split': self.min_samples_split,\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self\n"
   ],
   "id": "6b6f3f0954a443d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train RandomForest (Custom Implementation)\n",
    "random_forest = tune_hyperparameters(RandomForest(), param_grid_random_forest_custom, X_train, y_train)\n",
    "random_forest.fit(X_train, y_train)\n",
    "custom_rf_predictions = random_forest.predict(X_test)\n",
    "\n",
    "sklearn_rf = tune_hyperparameters(RandomForestClassifier(), param_grid_random_forest, X_train, y_train)\n",
    "sklearn_rf.fit(X_train, y_train)\n",
    "sklearn_rf_predictions = sklearn_rf.predict(X_test)\n",
    "\n",
    "evaluate_model(random_forest, X_train, y_train, X_test, y_test)\n",
    "evaluate_model(sklearn_rf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))  # Create a grid of Axes\n",
    "axes = axes.ravel()  # Flatten the array of Axes for easier indexing\n",
    "\n",
    "# Plot decision boundaries for 5 individual trees\n",
    "for i in range(5):\n",
    "    plot_decision_boundary(random_forest.trees[i], X_test, y_test, f\"Tree {i + 1}\", axes[i])\n",
    "\n",
    "# Plot decision boundary for Custom Random Forest\n",
    "plot_decision_boundary(random_forest, X_test, y_test, \"Custom Random Forest\", axes[5])\n",
    "\n",
    "# Sklearn Random Forest in a separate plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))  # Create a new figure and Axes\n",
    "plot_decision_boundary(sklearn_rf, X_test, y_test, \"Sklearn Random Forest\", ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "6a868665739a4842",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# phase 3",
   "id": "d26340fdbe0d2632"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "adaboost = tune_hyperparameters(AdaBoostClassifier(algorithm='SAMME'), param_grid_adaboost, X_train, y_train)\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "def plot_adaboost_stage(X, y, adaboost, stage, ax):\n",
    "    \"\"\"\n",
    "    Visualizes the decision boundary at a specific stage of AdaBoost.\n",
    "    Only considers weak learners up to the given stage.\n",
    "    \"\"\"\n",
    "    # Aggregate predictions up to the specified stage\n",
    "    stage_estimators = adaboost.estimators_[:stage]  # Get weak learners up to the stage\n",
    "    stage_weights = adaboost.estimator_weights_[:stage]  # Get weights up to the stage\n",
    "\n",
    "    def partial_predict(X):\n",
    "        \"\"\"Partial predictions up to a specific stage.\"\"\"\n",
    "        final_prediction = np.zeros(X.shape[0])\n",
    "        for estimator, weight in zip(stage_estimators, stage_weights):\n",
    "            final_prediction += weight * estimator.predict(X)\n",
    "        return np.sign(final_prediction)  # Return sign of weighted sum\n",
    "\n",
    "    # Decision boundary\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                         np.arange(y_min, y_max, 0.01))\n",
    "    Z = partial_predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot results\n",
    "    ax.contourf(xx, yy, Z, alpha=0.8, cmap='Paired')\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, edgecolor=\"k\", cmap='Paired')\n",
    "    ax.set_title(f\"Stage {stage}\")\n",
    "\n",
    "\n",
    "# Visualize 8 selected stages\n",
    "stages = [1, 5, 10, 15, 20, 30, 40, 50]\n",
    "_, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, stage in enumerate(stages):\n",
    "    plot_adaboost_stage(X_train, y_train, adaboost, stage, axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "evaluate_model(adaboost, X_train, y_train, X_test, y_test)\n"
   ],
   "id": "2b00a49a098a5df9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Phase 4 & extra point",
   "id": "8e60ba343c67415b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define Base Classifiers\n",
    "classifiers = {'Random Forest': RandomForestClassifier(random_state=42),\n",
    "               'SVM': SVC(probability=True, random_state=42),\n",
    "               'Logistic Regression': LogisticRegression(random_state=42),\n",
    "               'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "               'Overfitted Tree': DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=42)}\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "tuned_classifiers = {}\n",
    "param_grids = {\n",
    "    'Random Forest': {'n_estimators': [10, 20, 30], 'max_depth': [None, 10, 20]},\n",
    "    'SVM': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
    "    'Logistic Regression': {'C': [0.01, 0.1, 1, 10]},\n",
    "    'Decision Tree': {'max_depth': [None, 10, 20], 'min_samples_split': [2, 5, 10]},\n",
    "    'Overfitted Tree': {'max_depth': [None], 'min_samples_split': [2]}\n",
    "}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    grid_search = GridSearchCV(clf, param_grids[name], cv=5, scoring='accuracy', n_jobs=4)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    tuned_classifiers[name] = grid_search.best_estimator_\n",
    "\n",
    "tuned_classifiers"
   ],
   "id": "ba75c728f4d25170",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_stacked_classifier(base_classifiers, meta_classifier, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Trains a stacked classifier and evaluates it on training and testing data.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    meta_features_train = np.zeros((X_train.shape[0], len(base_classifiers)))\n",
    "    meta_features_test = np.zeros((X_test.shape[0], len(base_classifiers)))\n",
    "\n",
    "    for i, (name, clf) in enumerate(base_classifiers.items()):\n",
    "        fold_predictions = np.zeros((X_test.shape[0], skf.get_n_splits()))\n",
    "        for j, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "            clf_clone = clone(clf)\n",
    "            clf_clone.fit(X_train[train_idx], y_train[train_idx])\n",
    "            if hasattr(clf_clone, \"predict_proba\"):\n",
    "                meta_features_train[val_idx, i] = clf_clone.predict_proba(X_train[val_idx])[:, 1]\n",
    "                fold_predictions[:, j] = clf_clone.predict_proba(X_test)[:, 1]\n",
    "            else:\n",
    "                meta_features_train[val_idx, i] = clf_clone.predict(X_train[val_idx])\n",
    "                fold_predictions[:, j] = clf_clone.predict(X_test)\n",
    "        meta_features_test[:, i] = fold_predictions.mean(axis=1)\n",
    "\n",
    "    # Train meta-classifier\n",
    "    meta_classifier.fit(meta_features_train, y_train)\n",
    "    train_predictions = meta_classifier.predict(meta_features_train)\n",
    "    test_predictions = meta_classifier.predict(meta_features_test)\n",
    "\n",
    "    # Evaluate on training data\n",
    "    train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "    train_f1 = f1_score(y_train, train_predictions)\n",
    "    train_precision = precision_score(y_train, train_predictions)\n",
    "    train_recall = recall_score(y_train, train_predictions)\n",
    "\n",
    "    # Evaluate on testing data\n",
    "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "    test_f1 = f1_score(y_test, test_predictions)\n",
    "    test_precision = precision_score(y_test, test_predictions)\n",
    "    test_recall = recall_score(y_test, test_predictions)\n",
    "\n",
    "    return {\n",
    "        'train': {\n",
    "            'accuracy': train_accuracy,\n",
    "            'f1': train_f1,\n",
    "            'precision': train_precision,\n",
    "            'recall': train_recall\n",
    "        },\n",
    "        'test': {\n",
    "            'accuracy': test_accuracy,\n",
    "            'f1': test_f1,\n",
    "            'precision': test_precision,\n",
    "            'recall': test_recall\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "# Meta Classifier\n",
    "meta_classifier = LogisticRegression(random_state=42)\n",
    "\n",
    "# Train Stacked Classifier\n",
    "metrics = train_stacked_classifier(tuned_classifiers, meta_classifier, X_train, y_train, X_test, y_test)\n",
    "metrics"
   ],
   "id": "6e7906d780ca15d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Accuracy base on n_estimators",
   "id": "44abade97ccda61a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize models\n",
    "max_estimators = 30\n",
    "estimators_range = range(1, max_estimators + 1)\n",
    "\n",
    "ada_accuracies = []\n",
    "bagging_accuracies = []\n",
    "rf_accuracies = []\n",
    "\n",
    "bagging_custom_accuracies = []\n",
    "rf_custom_accuracies = []\n",
    "\n",
    "# Train and evaluate each model for different numbers of estimators\n",
    "for n_estimators in estimators_range:\n",
    "    # AdaBoost\n",
    "    ada = AdaBoostClassifier(\n",
    "        estimator=DecisionTreeClassifier(),\n",
    "        n_estimators=n_estimators,\n",
    "        random_state=42,\n",
    "        algorithm='SAMME',\n",
    "    )\n",
    "    ada.fit(X_train, y_train)\n",
    "    ada_preds = ada.predict(X_test)\n",
    "    ada_accuracies.append(accuracy_score(y_test, ada_preds))\n",
    "\n",
    "    # Bagging\n",
    "    bagging = BaggingClassifier(\n",
    "        estimator=RandomForestClassifier(),\n",
    "        n_estimators=n_estimators,\n",
    "        random_state=42,\n",
    "        bootstrap=True, max_features=1.0, max_samples=0.5\n",
    "    )\n",
    "    bagging.fit(X_train, y_train)\n",
    "    bagging_preds = bagging.predict(X_test)\n",
    "    bagging_accuracies.append(accuracy_score(y_test, bagging_preds))\n",
    "\n",
    "    bagging_custom = Bagging(base_model=RandomForestClassifier,\n",
    "                             n_estimators=n_estimators,\n",
    "                             random_state=42)\n",
    "    bagging_custom.fit(X_train, y_train)\n",
    "    bagging_custom_preds = bagging_custom.predict(X_test)\n",
    "    bagging_custom_accuracies.append(accuracy_score(y_test, bagging_custom_preds))\n",
    "\n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        random_state=42,\n",
    "        bootstrap=True, max_depth=10, max_features='sqrt', min_samples_leaf=2, min_samples_split=2\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_preds = rf.predict(X_test)\n",
    "    rf_accuracies.append(accuracy_score(y_test, rf_preds))\n",
    "\n",
    "    rf_custom = RandomForest(\n",
    "        n_estimators=n_estimators,\n",
    "        random_state=42\n",
    "    )\n",
    "    rf_custom.fit(X_train, y_train)\n",
    "    rf_custom_preds = rf_custom.predict(X_test)\n",
    "    rf_custom_accuracies.append(accuracy_score(y_test, rf_custom_preds))\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(estimators_range, ada_accuracies, label='AdaBoost', marker='o')\n",
    "plt.plot(estimators_range, bagging_accuracies, label='Bagging', marker='s')\n",
    "plt.plot(estimators_range, rf_accuracies, label='Random Forest', marker='^')\n",
    "\n",
    "plt.plot(estimators_range, bagging_custom_accuracies, label='Bagging custom', marker='s')\n",
    "plt.plot(estimators_range, rf_custom_accuracies, label='Random Forest custom', marker='^')\n",
    "\n",
    "plt.title('Accuracy vs. Number of Estimators')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "bfb22ea5953a0e7b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
